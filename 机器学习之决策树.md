
@[TOC](目录)

 * * *

## 1.什么是决策树/判定树(decision tree)
决策树是一个类似于流程图 的树结构，决策树由有向边和结点构成，决策树的结点有两种，一种是内部结点，一种是叶结点。分类决策树中，内部结点代表特征，叶结点代表类。回归树中，内部结点代表输入空间的划分区域，叶结点代表输出值。每个内部节点表示在一个属性上的测试，每个分支代表一个属性输出，而每个树叶节点代表类或类分布
决策树是一种既可以用于分类又可用于回归的算法
决策树可以认为是if-then规则，也可认为是输入空间和类空间条件概率
决策树的生成分为三步：特征值的选择，决策树生成，决策树剪枝
![图1 决策树](https://img-blog.csdnimg.cn/20190609164301193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
## 2. 基本概念

### 2.1 熵(entropy)
熵是表达随机变量不确定性的一种度量。对于随机变量X，P(X=xi)=pi定义
  ![在这里插入图片描述](https://img-blog.csdnimg.cn/2019060917160130.png#pic_center)
当对数以2为底时，单位为比特1bit=log2^29(log2以2为底)，当对数以e为底时单位为纳特。由定义可知，熵只和随机变量X的分布有关和X的取值无关。
例如：假设世界杯32支球队的概率相同，都为1/32，可计算
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609172313423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
即说明可能需要经过5次的筛选（折半：16，8，4，2，1）最后得到结果（一支球队能否夺冠）
### 2.2 其他熵
更多熵概念信息如图所示：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609173130738.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
## 3. 决策树归纳算法 (ID3)
### 3.1 信息获取量(Information Gain)
Gain(A)=Info(D)-Info_A(D),代表通过节点A进行分类获取了多少信息（单位是bit）
解读：Info(D)代表仅仅按照标记来分的时候的所有信息熵
    	Info_A(D)代表以特征A来分时的信息熵
    	所以Gain(A)=Info(D)-Info_A(D)为二者差值的信息熵
![](https://img-blog.csdnimg.cn/20190609183002641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
不考虑特征age,income,student,credit_rating的情况下，根据Class:buys_computer标记yes/no计算出的信息熵：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609183038728.png#pic_center)
根据age特征计算出的信息熵：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609183106424.png#pic_center)
两者的差值：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609183810651.png#pic_center)
同理可计算出根据income,student,credit_rating的信息获取量为：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609183942384.png#pic_center)
选择最大的信息获取量Gain(age)=0.246作为第一个根节点：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609184201918.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
注：在训练集表中可以看到，age有三种分类，所以上图中以age为根节点的树中age下有三个分支

通过观察可知，middle_aged分支下class全部都是yes，所以没有必要在进行分支。而youth和senior分支下class还存在yes和no，需要进行递归处理。方法同上
不断重复这个过程，直到所有的叶子节点下class均为yes或者no
对算法的描述如下图：
![在这里插入图片描述](https://img-blog.csdnimg.cn/20190609185416432.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMzMjA4ODUx,size_16,color_FFFFFF,t_70#pic_center)
### 3.2 其他算法
相同点：都是贪心算法，自上而下
不同点：属性选择 度量方法不同。C4.5(gain ratio),CART(gini index),ID3(Information Gain)
如何处理连续型变量的属性？设置阈值


## 4. 使用示例

## 5. 总结
优点：直观，便于理解，小规模数据集有效
缺点：处理连续型变量效果不好，类别较多时错误增长得较快，规模性一般


[决策树相关文章](https://blog.csdn.net/K_Albert/article/details/78290517)
